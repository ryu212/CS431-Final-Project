{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13798380,"sourceType":"datasetVersion","datasetId":8738954},{"sourceId":13835388,"sourceType":"datasetVersion","datasetId":8811429}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install diffusers\n%pip install peft\n!pip install protobuf==3.20.*\n!pip install clean-fid\n!pip install open_clip_torch\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nfrom diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, DiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import login\nfrom peft import LoraConfig\nimport torch\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport math\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom peft.utils import get_peft_model_state_dict\nfrom diffusers.utils import convert_state_dict_to_diffusers\nfrom datasets import load_dataset\nfrom functools import partial\nfrom PIL import Image\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom pydantic import BaseModel\nfrom diffusers.training_utils import compute_snr\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:50:15.751355Z","iopub.execute_input":"2025-11-23T06:50:15.751677Z","iopub.status.idle":"2025-11-23T06:50:40.894368Z","shell.execute_reply.started":"2025-11-23T06:50:15.751655Z","shell.execute_reply":"2025-11-23T06:50:40.893516Z"}},"outputs":[{"name":"stderr","text":"2025-11-23 06:50:25.032809: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763880625.225711      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763880625.282125      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"df=pd.read_csv('/kaggle/input/fashion-dataset/val_fixed.csv')\ndf.head()\ndf = df.iloc[:2000]\ncaptions=df['caption'].tolist()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:50:49.573528Z","iopub.execute_input":"2025-11-23T06:50:49.574162Z","iopub.status.idle":"2025-11-23T06:50:49.605502Z","shell.execute_reply.started":"2025-11-23T06:50:49.574134Z","shell.execute_reply":"2025-11-23T06:50:49.604743Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import os\nimport shutil\n\ndef copy_first_2000_images(source_folder, destination_folder):\n    \"\"\"\n    Sao chép 2000 ảnh đầu tiên từ thư mục nguồn sang thư mục đích\n    \"\"\"\n    # Tạo thư mục đích nếu chưa tồn tại\n    if not os.path.exists(destination_folder):\n        os.makedirs(destination_folder)\n    \n    # Lấy danh sách tất cả file ảnh trong thư mục nguồn\n    all_images = [f for f in os.listdir(source_folder) \n                 if f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n    \n    # Sắp xếp danh sách ảnh theo tên\n    all_images.sort()\n    \n    # Lấy 2000 ảnh đầu tiên\n    first_2000_images = all_images[:2000]\n    \n    # Sao chép 2000 ảnh sang thư mục đích\n    for image_name in first_2000_images:\n        source_path = os.path.join(source_folder, image_name)\n        destination_path = os.path.join(destination_folder, image_name)\n        shutil.copy2(source_path, destination_path)\n    \n    print(f\"Đã sao chép {len(first_2000_images)} ảnh vào thư mục: {destination_folder}\")\n    return first_2000_images\n\n# Sử dụng hàm - SAO CHÉP thay vì di chuyển\nsource_folder = '/kaggle/input/fashion-dataset/val_images'\ndestination_folder = '/kaggle/working/real_images'\n\nimages_list = copy_first_2000_images(source_folder, destination_folder)\n\n# In ra thông tin\nprint(f\"\\nTổng số ảnh đã sao chép: {len(images_list)}\")\nprint(\"\\n10 ảnh đầu tiên:\")\nfor i, img in enumerate(images_list[:10]):\n    print(f\"{i+1}. {img}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport torch\nfrom PIL import Image\nimport os\n\ndevice = \"cuda\"\n\npipe = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\",\n    torch_dtype=torch.float16\n).to(device)\n\nsave_gen_dir = \"generated_images\"\nos.makedirs(save_gen_dir, exist_ok=True)\n\ngenerated_paths = []\n\nfor i, prompt in enumerate(captions):\n    img = pipe(prompt, num_inference_steps=40).images[0]\n    save_path = f\"{save_gen_dir}/{i:04d}.png\"\n    img.save(save_path)\n    generated_paths.append(save_path)\n\nprint(\"Generated:\", len(generated_paths))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-22T14:21:35.889780Z","iopub.execute_input":"2025-11-22T14:21:35.890071Z","iopub.status.idle":"2025-11-22T14:22:39.869923Z","shell.execute_reply.started":"2025-11-22T14:21:35.890051Z","shell.execute_reply":"2025-11-22T14:22:39.868887Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from cleanfid import fid\n\nfid_score = fid.compute_fid(\n    \"/kaggle/input/fidclip-sd1-5-base-generated-images/real_images\",\n    \"/kaggle/input/fidclip-sd1-5-base-generated-images/generated_images\",\n    mode=\"clean\"\n)\n\n\nprint(\"FID score:\", fid_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:40:50.161651Z","iopub.execute_input":"2025-11-23T06:40:50.162392Z","iopub.status.idle":"2025-11-23T06:43:17.999466Z","shell.execute_reply.started":"2025-11-23T06:40:50.162365Z","shell.execute_reply":"2025-11-23T06:43:17.998470Z"}},"outputs":[{"name":"stdout","text":"compute FID between two folders\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 12 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Found 2000 images in the folder /kaggle/input/fidclip-sd1-5-base-generated-images/real_images\n","output_type":"stream"},{"name":"stderr","text":"FID real_images : 100%|██████████| 63/63 [01:51<00:00,  1.76s/it]\n","output_type":"stream"},{"name":"stdout","text":"Found 2000 images in the folder /kaggle/input/fidclip-sd1-5-base-generated-images/generated_images\n","output_type":"stream"},{"name":"stderr","text":"FID generated_images : 100%|██████████| 63/63 [00:21<00:00,  2.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"FID score: 62.03515192696102\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport torch\nimport open_clip\nfrom PIL import Image\nimport numpy as np\n\n# ==============================\n# 1. Load CLIP model\n# ==============================\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nmodel, _, preprocess = open_clip.create_model_and_transforms(\n    'ViT-B-32',\n    pretrained='openai'\n)\nmodel = model.to(device)\n\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\n# ==============================\n# 2. Tạo generated_paths từ thư mục ảnh\n# ==============================\n\ngenerated_dir = \"/kaggle/input/fidclip-sd1-5-base-generated-images/generated_images\"   # <<< sửa theo folder của bạn\n\ngenerated_paths = sorted([\n    os.path.join(generated_dir, f)\n    for f in os.listdir(generated_dir)\n    if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n])\n\nprint(\"Số ảnh tìm thấy:\", len(generated_paths))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:52:08.488452Z","iopub.execute_input":"2025-11-23T06:52:08.488854Z","iopub.status.idle":"2025-11-23T06:52:10.520680Z","shell.execute_reply.started":"2025-11-23T06:52:08.488822Z","shell.execute_reply":"2025-11-23T06:52:10.520040Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/open_clip/factory.py:450: UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Số ảnh tìm thấy: 2000\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"\n\n# ==============================\n# 3. Load captions (đã có sẵn)\n# ==============================\n# captions = [...]  # bạn đã có list này từ CSV\n\nassert len(generated_paths) == len(captions), (\n    f\"Không khớp số lượng! {len(generated_paths)} ảnh nhưng {len(captions)} captions\"\n)\n\nclip_scores = []\n\n# ==============================\n# 4. Loop tính CLIP score hình ↔ caption\n# ==============================\nfor img_path, prompt in zip(generated_paths, captions):\n\n    image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n    text = tokenizer([prompt]).to(device)\n\n    with torch.no_grad():\n        img_feat = model.encode_image(image)\n        txt_feat = model.encode_text(text)\n\n    img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n    txt_feat = txt_feat / txt_feat.norm(dim=-1, keepdim=True)\n\n    score = (img_feat @ txt_feat.T).item()\n    clip_scores.append(score)\n\n# ==============================\n# 5. Output\n# ==============================\navg_score = float(np.mean(clip_scores))\nprint(\"Số lượng samples:\", len(clip_scores))\nprint(\"Average CLIP score:\", avg_score)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T06:52:17.081589Z","iopub.execute_input":"2025-11-23T06:52:17.082318Z","iopub.status.idle":"2025-11-23T06:53:12.994643Z","shell.execute_reply.started":"2025-11-23T06:52:17.082285Z","shell.execute_reply":"2025-11-23T06:53:12.993946Z"}},"outputs":[{"name":"stdout","text":"Số lượng samples: 2000\nAverage CLIP score: 0.3164151344001293\n","output_type":"stream"}],"execution_count":13}]}