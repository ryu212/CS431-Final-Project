{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13788386,"sourceType":"datasetVersion","datasetId":8777800},{"sourceId":13798380,"sourceType":"datasetVersion","datasetId":8738954}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install diffusers\n%pip install peft","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:42:32.542342Z","iopub.execute_input":"2025-11-19T07:42:32.542568Z","iopub.status.idle":"2025-11-19T07:43:59.603116Z","shell.execute_reply.started":"2025-11-19T07:42:32.542541Z","shell.execute_reply":"2025-11-19T07:43:59.602004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install protobuf==3.20.*","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:46:58.923872Z","iopub.execute_input":"2025-11-19T07:46:58.924234Z","iopub.status.idle":"2025-11-19T07:47:03.237050Z","shell.execute_reply.started":"2025-11-19T07:46:58.924199Z","shell.execute_reply":"2025-11-19T07:47:03.236012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nfrom diffusers import UNet2DConditionModel, AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, DiffusionPipeline\nfrom transformers import CLIPTextModel, CLIPTokenizer\nfrom huggingface_hub import login\nfrom peft import LoraConfig\nimport torch\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport torch.nn.functional as F\nimport math\nfrom tqdm.auto import tqdm\nimport matplotlib.pyplot as plt\nfrom peft.utils import get_peft_model_state_dict\nfrom diffusers.utils import convert_state_dict_to_diffusers\nfrom datasets import load_dataset\nfrom functools import partial\nfrom PIL import Image\nfrom kaggle_secrets import UserSecretsClient\nfrom torch.utils.data import Dataset\nimport pandas as pd\nfrom pydantic import BaseModel\nfrom diffusers.training_utils import compute_snr\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T07:47:36.417747Z","iopub.execute_input":"2025-11-19T07:47:36.418283Z","iopub.status.idle":"2025-11-19T07:48:18.419110Z","shell.execute_reply.started":"2025-11-19T07:47:36.418253Z","shell.execute_reply":"2025-11-19T07:48:18.418527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"login(token=UserSecretsClient().get_secret(\"Longphanryu\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:32:57.889826Z","iopub.execute_input":"2025-11-18T19:32:57.890517Z","iopub.status.idle":"2025-11-18T19:32:58.213038Z","shell.execute_reply.started":"2025-11-18T19:32:57.890483Z","shell.execute_reply":"2025-11-18T19:32:58.212224Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_models(model_name: str, dtype=torch.float16):\n    \"\"\"\n    Load tokenizer, text_encoder, vae, scheduler, unet.\n    Trả về các model ở dtype (thường float16 cho T4), nhưng chúng ta sẽ chuyển\n    trainable params sang float32 sau khi thêm LoRA.\n    \"\"\"\n    tokenizer = CLIPTokenizer.from_pretrained(model_name, subfolder=\"tokenizer\")\n    # Sử dụng torch_dtype nếu environment hỗ trợ (giảm VRAM)\n    text_encoder = CLIPTextModel.from_pretrained(model_name, subfolder=\"text_encoder\")\n    vae = AutoencoderKL.from_pretrained(model_name, subfolder=\"vae\")\n    scheduler = DDPMScheduler.from_pretrained(model_name, subfolder=\"scheduler\")\n    unet = UNet2DConditionModel.from_pretrained(model_name, subfolder=\"unet\")\n\n    # Chuyển các module lớn sang dtype để giảm VRAM (đặc biệt trên T4)\n    text_encoder.to(dtype=dtype)\n    vae.to(dtype=dtype)\n    unet.to(dtype=dtype)\n\n    return tokenizer, text_encoder, vae, scheduler, unet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:33:01.124300Z","iopub.execute_input":"2025-11-18T19:33:01.124566Z","iopub.status.idle":"2025-11-18T19:33:01.130201Z","shell.execute_reply.started":"2025-11-18T19:33:01.124544Z","shell.execute_reply":"2025-11-18T19:33:01.129399Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class FashionDataset(Dataset):\n    def __init__(self, csv_path: str, image_folder: str, tokenizer: CLIPTokenizer, image_size: int = 512):\n        self.image_folder = image_folder\n        self.tokenizer = tokenizer\n\n        # Load CSV\n        self.df = pd.read_csv(csv_path)\n\n        # 1) Lọc caption null / empty\n        self.df = self.df.dropna(subset=[\"caption\"])\n        self.df = self.df[self.df[\"caption\"].apply(lambda x: isinstance(x, str) and len(x.strip()) > 0)]\n\n        # 2) Lọc ảnh không tồn tại\n        self.df = self.df[self.df[\"image\"].apply(lambda x: os.path.exists(os.path.join(image_folder, x)))]\n\n        # 3) Reset index\n        self.df = self.df.reset_index(drop=True)\n\n        # 4) Transforms - sửa normalize cho 3 kênh RGB\n        self.transforms = transforms.Compose([\n            transforms.Resize((image_size, image_size)),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n        ])\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.image_folder, row[\"image\"])\n        try:\n            image = Image.open(img_path).convert(\"RGB\")\n        except Exception as e:\n            raise FileNotFoundError(f\"Cannot open image: {img_path}. Exception: {e}\")\n\n        image = self.transforms(image)\n        caption = row[\"caption\"]\n\n        input_ids = self.tokenizer(\n            caption,\n            max_length=self.tokenizer.model_max_length,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\"\n        )[\"input_ids\"][0]\n\n        return {\n            \"pixel_values\": image,\n            \"input_ids\": input_ids,\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T08:13:25.286703Z","iopub.execute_input":"2025-11-19T08:13:25.287178Z","iopub.status.idle":"2025-11-19T08:13:25.295277Z","shell.execute_reply.started":"2025-11-19T08:13:25.287153Z","shell.execute_reply":"2025-11-19T08:13:25.294696Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_lora_params(model: torch.nn.Module):\n    \"\"\"\n    Lấy parameters có requires_grad=True — giả sử adapter/LoRA đã bật grad cho các tham số đó.\n    \"\"\"\n    return [p for p in model.parameters() if p.requires_grad]\n\ndef enable_lora_trainable_params(module: torch.nn.Module):\n    \"\"\"\n    Sau khi gọi add_adapter, nhiều implementation sẽ thêm tham số với tên chứa 'lora' hoặc 'adapter'.\n    Hàm này sẽ bật requires_grad cho các tham số có tên chứa một số từ khoá thường gặp.\n    Nếu implementation của bạn khác, chỉnh lại điều kiện tìm kiếm tên tham số.\n    \"\"\"\n    keywords = [\"lora\", \"adapter\", \"lora_up\", \"lora_down\", \"alpha\"]\n    matched = []\n    for name, p in module.named_parameters():\n        if any(k in name.lower() for k in keywords):\n            p.requires_grad = True\n            matched.append(name)\n        else:\n            p.requires_grad = False\n    # Trả về danh sách tên param được bật\n    return matched\n\n# -------------------------\n# Setup models for training\n# -------------------------\ndef setup_models_for_training(model_name: str, rank: int = 128, dtype=torch.float16, device=None):\n    tokenizer, text_encoder, vae, scheduler, unet = get_models(model_name, dtype=dtype)\n\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # 1) Freeze VAE hoàn toàn (không train)\n    vae.eval()\n    for p in vae.parameters():\n        p.requires_grad = False\n\n    # 2) Freeze base weights của text_encoder + unet trước\n    for p in text_encoder.parameters():\n        p.requires_grad = False\n    for p in unet.parameters():\n        p.requires_grad = False\n\n    # 3) Thêm LoRA/adapter config cho UNet và Text Encoder\n    unet_lora_config = LoraConfig(\n        r=rank,\n        lora_alpha=rank,\n        init_lora_weights=\"gaussian\",\n        target_modules=[\"to_k\", \"to_q\", \"to_v\", \"to_out.0\"],  # giữ như bạn\n    )\n    # Thêm adapter (hàm này phụ thuộc implementation của bạn)\n    # ví dụ: unet.add_adapter(unet_lora_config)\n    unet.add_adapter(unet_lora_config)\n\n    text_encoder_lora_config = LoraConfig(\n        r=rank,\n        lora_alpha=rank,\n        init_lora_weights=\"gaussian\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n    )\n    text_encoder.add_adapter(text_encoder_lora_config)\n\n    # 4) Bật requires_grad cho tham số LoRA/adapter (dựa trên tên param)\n    unet_matched = enable_lora_trainable_params(unet)\n    text_matched = enable_lora_trainable_params(text_encoder)\n\n    # 5) Đảm bảo các param trainable ở float32 (ổn định khi dùng LoRA)\n    for p in unet.parameters():\n        if p.requires_grad:\n            p.data = p.data.to(torch.float32)\n    for p in text_encoder.parameters():\n        if p.requires_grad:\n            p.data = p.data.to(torch.float32)\n\n    # 6) Đưa các module chính sang device (VAE ở eval trên device)\n    vae.to(device).eval()\n    text_encoder.to(device)  # will be used for LoRA training (set train mode later)\n    unet.to(device).train()\n\n    # Thông tin debug\n    print(f\"LoRA params in UNet matched: {len(unet_matched)} names (examples): {unet_matched[:10]}\")\n    print(f\"LoRA params in TextEncoder matched: {len(text_matched)} names (examples): {text_matched[:10]}\")\n\n    return tokenizer, text_encoder, vae, scheduler, unet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:33:16.840475Z","iopub.execute_input":"2025-11-18T19:33:16.841079Z","iopub.status.idle":"2025-11-18T19:33:16.851224Z","shell.execute_reply.started":"2025-11-18T19:33:16.841054Z","shell.execute_reply":"2025-11-18T19:33:16.850458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TrainingConfig:\n    def __init__(self,\n                 train_steps: int = 100,\n                 lr: float = 1e-5,\n                 batch_size: int = 2,\n                 accumulation_steps: int = 4,\n                 rank: int = 128,\n                 max_grad_norm: float = 1.0,\n                 pretrained_name: str = \"runwayml/stable-diffusion-v1-5\",\n                 csv_path: str = \"/kaggle/input/fashion-dataset/train_fixed.csv\",\n                 image_folder: str = \"/kaggle/input/fashion-dataset/train_images\",\n                 snr_gamma: float = -1,\n                 seed: int = 42):\n        self.train_steps = train_steps\n        self.lr = lr\n        self.batch_size = batch_size\n        self.accumulation_steps = accumulation_steps\n        self.rank = rank\n        self.max_grad_norm = max_grad_norm\n        self.pretrained_name = pretrained_name\n        self.csv_path = csv_path\n        self.image_folder = image_folder\n        self.snr_gamma = snr_gamma\n        self.seed = seed","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:33:21.284535Z","iopub.execute_input":"2025-11-18T19:33:21.285209Z","iopub.status.idle":"2025-11-18T19:33:21.290659Z","shell.execute_reply.started":"2025-11-18T19:33:21.285184Z","shell.execute_reply":"2025-11-18T19:33:21.289643Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def compute_snr(scheduler: DDPMScheduler, timesteps: torch.Tensor):\n    \"\"\"Compute SNR for loss weighting.\"\"\"\n    alpha_prod = scheduler.alphas_cumprod.to(timesteps.device)\n    alpha = alpha_prod[timesteps]\n    snr = alpha / (1 - alpha)\n    return snr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:33:24.223503Z","iopub.execute_input":"2025-11-18T19:33:24.224226Z","iopub.status.idle":"2025-11-18T19:33:24.228443Z","shell.execute_reply.started":"2025-11-18T19:33:24.224197Z","shell.execute_reply":"2025-11-18T19:33:24.227495Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train(\n    tokenizer: CLIPTokenizer,\n    text_encoder: CLIPTextModel,\n    vae: AutoencoderKL,\n    scheduler: DDPMScheduler,\n    unet: UNet2DConditionModel,\n    config: TrainingConfig,\n    device=None\n):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Gộp các tham số LoRA từ unet + text_encoder\n    lora_params = get_lora_params(unet) + get_lora_params(text_encoder)\n    if len(lora_params) == 0:\n        raise RuntimeError(\"Không có tham số trainable (LoRA). Kiểm tra add_adapter / enable_lora_trainable_params.\")\n\n    # Put models to device / mode\n    vae.to(device).eval()\n    text_encoder.to(device).train()\n    unet.to(device).train()\n\n    # Dataset / Dataloader\n    train_dataset = FashionDataset(config.csv_path, config.image_folder, tokenizer)\n    train_dataloader = DataLoader(\n        train_dataset,\n        batch_size=config.batch_size,\n        shuffle=True,\n        num_workers=2,\n        pin_memory=True,\n        drop_last=True\n    )\n\n    # Compute steps/epochs\n    steps_per_epoch = math.ceil(len(train_dataloader) / config.accumulation_steps)\n    epochs = math.ceil(config.train_steps / steps_per_epoch)\n\n    # Adjust lr to account for accumulation & batch (kept similar to original)\n    lr = config.lr * config.accumulation_steps * config.batch_size\n    optimizer = torch.optim.AdamW(lora_params, lr=lr)\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    global_step = 0\n    progress_bar = tqdm(range(config.train_steps), desc=\"Steps\")\n\n    print(\"Training config:\")\n    print(f\" train_steps: {config.train_steps}, epochs (est): {epochs}, steps_per_epoch: {steps_per_epoch}\")\n    print(f\" batch_size: {config.batch_size}, accumulation_steps: {config.accumulation_steps}, effective lr: {lr}\")\n\n    losses = []\n    torch.manual_seed(config.seed)\n    for epoch in range(epochs):\n        for step, batch in enumerate(train_dataloader):\n            bs = batch[\"input_ids\"].shape[0]\n\n            with torch.autocast(device_type=\"cuda\" if device.type == \"cuda\" else \"cpu\", dtype=torch.float16):\n                # Text encoder forward (không dùng torch.no_grad vì LoRA text encoder cần grad)\n                encoder_hidden_states = text_encoder(batch[\"input_ids\"].to(device), return_dict=False)[0]\n\n                # Encode images to latents (VAE frozen so no grad)\n                with torch.no_grad():\n                    latents = vae.encode(batch[\"pixel_values\"].to(device)).latent_dist.sample()\n                    latents = latents * vae.config.scaling_factor\n\n                # Noise & timesteps\n                timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (bs,), device=device).long()\n                noise = torch.randn_like(latents)\n                noisy_latents = scheduler.add_noise(latents, noise, timesteps)\n\n                # UNet predicts noise\n                noise_pred = unet(noisy_latents, timesteps, encoder_hidden_states, return_dict=False)[0]\n\n                # Loss (MSE or SNR-weighted)\n                if config.snr_gamma > 0:\n                    snr = compute_snr(scheduler, timesteps)\n                    mse_loss_weights = torch.stack([snr, config.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0]\n                    mse_loss_weights = mse_loss_weights / snr\n                    loss = F.mse_loss(noise_pred, noise, reduction=\"none\")\n                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights\n                    loss = loss.mean()\n                else:\n                    loss = F.mse_loss(noise_pred, noise, reduction=\"mean\")\n\n            global_step += 1\n            scaler.scale(loss).backward()\n\n            if global_step % config.accumulation_steps == 0:\n                # Gradient clipping (unscale first)\n                if config.max_grad_norm > 0:\n                    scaler.unscale_(optimizer)\n                    torch.nn.utils.clip_grad_norm_(lora_params, config.max_grad_norm)\n                scaler.step(optimizer)\n                scaler.update()\n                optimizer.zero_grad()\n                progress_bar.update(1)\n\n            losses.append(loss.item())\n            progress_bar.set_postfix({\"loss\": losses[-1]})\n\n            # stop condition\n            if global_step / config.accumulation_steps >= config.train_steps:\n                break\n        # End epoch\n    return {\"losses\": losses}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:33:26.394409Z","iopub.execute_input":"2025-11-18T19:33:26.395117Z","iopub.status.idle":"2025-11-18T19:33:26.411551Z","shell.execute_reply.started":"2025-11-18T19:33:26.395083Z","shell.execute_reply":"2025-11-18T19:33:26.410786Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_lora_weights(unet, text_encoder, save_path=\"lora.safetensors\"):\n    from safetensors.torch import save_file\n\n    # gom tất cả param trainable (LoRA) của UNet + TextEncoder\n    lora_state = {}\n\n    for name, param in unet.named_parameters():\n        if param.requires_grad:\n            lora_state[f\"unet.{name}\"] = param.detach().cpu()\n\n    for name, param in text_encoder.named_parameters():\n        if param.requires_grad:\n            lora_state[f\"text_encoder.{name}\"] = param.detach().cpu()\n\n    save_file(lora_state, save_path)\n    print(f\"✔ Saved LoRA weights → {save_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:33:30.922586Z","iopub.execute_input":"2025-11-18T19:33:30.922881Z","iopub.status.idle":"2025-11-18T19:33:30.928204Z","shell.execute_reply.started":"2025-11-18T19:33:30.922859Z","shell.execute_reply":"2025-11-18T19:33:30.927329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    cfg = TrainingConfig(\n        train_steps=10,        # điều chỉnh tuỳ bạn\n        lr=1e-5,\n        batch_size=2,\n        accumulation_steps=4,\n        rank=64,                # giảm rank nếu VRAM hạn chế trên T4\n        max_grad_norm=1.0,\n        pretrained_name=\"runwayml/stable-diffusion-v1-5\",\n        csv_path=\"/kaggle/input/fashion-dataset/train_fixed.csv\",\n        image_folder=\"/kaggle/input/fashion-dataset/train_images\",\n        snr_gamma=-1,\n        seed=42\n    )\n\n    # Load & setup\n    tokenizer, text_encoder, vae, scheduler, unet = setup_models_for_training(\n        cfg.pretrained_name,\n        rank=cfg.rank,\n        dtype=torch.float16,\n        device=device\n    )\n\n    # Train\n    metrics = train(tokenizer, text_encoder, vae, scheduler, unet, cfg, device=device)\n    save_lora_weights(unet, text_encoder, \"my_lora.safetensors\")\n    print(\"Done. Final losses (last 5):\", metrics[\"losses\"][-5:])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:35:58.063041Z","iopub.execute_input":"2025-11-18T19:35:58.063356Z","iopub.status.idle":"2025-11-18T19:36:42.123775Z","shell.execute_reply.started":"2025-11-18T19:35:58.063328Z","shell.execute_reply":"2025-11-18T19:36:42.123006Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"===============================================================","metadata":{}},{"cell_type":"code","source":"!pip install diffusers transformers accelerate torch torchvision pillow pandas numpy tqdm scipy ftfy open_clip_torch safetensors\n!pip install pytorch-fid\n!pip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T08:11:37.553558Z","iopub.execute_input":"2025-11-19T08:11:37.554102Z","iopub.status.idle":"2025-11-19T08:11:53.105076Z","shell.execute_reply.started":"2025-11-19T08:11:37.554077Z","shell.execute_reply":"2025-11-19T08:11:53.104311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport os\nfrom tqdm import tqdm\nimport csv\n\n# For FID calculation\nfrom scipy import linalg\n\n# For CLIP Score\nimport clip\n\n# Dataset class\nclass ValidationDataset(Dataset):\n    def __init__(self, csv_file, image_folder, transform=None):\n        self.data = pd.read_csv(csv_file)\n        self.image_folder = image_folder\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        img_name = self.data.iloc[idx]['image']  # Your image column name\n        caption = self.data.iloc[idx]['caption']  # Your caption column name\n        \n        img_path = os.path.join(self.image_folder, img_name)\n        image = Image.open(img_path).convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        return image, caption, img_name\n\n# Load model with LoRA for both UNet and text encoder\ndef load_model_with_lora():\n    from diffusers import StableDiffusionPipeline\n    from safetensors.torch import load_file\n    \n    # Load base model\n    model_id = \"runwayml/stable-diffusion-v1-5\"  # Adjust as needed\n    pipe = StableDiffusionPipeline.from_pretrained(\n        model_id,\n        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n        safety_checker=None,\n        requires_safety_checker=False,\n    )\n    \n    # Load LoRA weights for both UNet and text encoder\n    lora_weights = load_file(\"/kaggle/input/lora-weight/my_lora.safetensors\")\n    \n    # Load UNet LoRA\n    pipe.unet.load_state_dict(lora_weights, strict=False)\n    \n    # Load text encoder LoRA if present\n    text_encoder_keys = [k for k in lora_weights.keys() if k.startswith('text_encoder')]\n    if text_encoder_keys:\n        pipe.text_encoder.load_state_dict(lora_weights, strict=False)\n    \n    if torch.cuda.is_available():\n        pipe = pipe.to(\"cuda\")\n    \n    return pipe\n\n# Generate images for validation\ndef generate_images(pipe, captions, output_dir, image_size=512):\n    os.makedirs(output_dir, exist_ok=True)\n    generated_paths = []\n    \n    for i, caption in enumerate(tqdm(captions, desc=\"Generating images\")):\n        # Generate image\n        with torch.autocast(\"cuda\"):\n            image = pipe(\n                caption, \n                num_inference_steps=50,\n                guidance_scale=7.5,\n                height=image_size,\n                width=image_size\n            ).images[0]\n        \n        # Save image\n        img_path = os.path.join(output_dir, f\"generated_{i:05d}.png\")\n        image.save(img_path)\n        generated_paths.append(img_path)\n    \n    return generated_paths\n\n# Calculate CLIP Score\ndef calculate_clip_score(model, preprocess, generated_images, captions, batch_size=16):\n    device = next(model.parameters()).device\n    \n    # Process images\n    image_tensors = []\n    for img_path in tqdm(generated_images, desc=\"Processing images for CLIP\"):\n        image = Image.open(img_path).convert('RGB')\n        image_tensor = preprocess(image).unsqueeze(0)\n        image_tensors.append(image_tensor)\n    \n    image_tensors = torch.cat(image_tensors).to(device)\n    \n    # Process texts\n    text_tokens = clip.tokenize(captions, truncate=True).to(device)\n    \n    # Calculate similarities in batches\n    similarities = []\n    for i in tqdm(range(0, len(image_tensors), batch_size), desc=\"Calculating CLIP scores\"):\n        batch_images = image_tensors[i:i+batch_size]\n        batch_texts = text_tokens[i:i+batch_size]\n        \n        with torch.no_grad():\n            image_features = model.encode_image(batch_images)\n            text_features = model.encode_text(batch_texts)\n            \n            # Normalize features\n            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n            text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n            \n            # Calculate cosine similarity\n            similarity = (image_features * text_features).sum(dim=-1)\n            similarities.append(similarity.cpu())\n    \n    similarities = torch.cat(similarities)\n    clip_score = similarities.mean().item()\n    \n    return clip_score, similarities\n\n# Extract features for FID calculation using InceptionV3\ndef extract_features_inception(image_paths, batch_size=16, dims=2048):\n    \"\"\"Extract features using InceptionV3 for FID calculation\"\"\"\n    from pytorch_fid.inception import InceptionV3\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n    model = InceptionV3([block_idx]).to(device)\n    model.eval()\n    \n    transform = transforms.Compose([\n        transforms.Resize((299, 299)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    all_features = []\n    \n    for i in tqdm(range(0, len(image_paths), batch_size), desc=\"Extracting features for FID\"):\n        batch_paths = image_paths[i:i+batch_size]\n        batch_images = []\n        \n        for path in batch_paths:\n            try:\n                image = Image.open(path).convert('RGB')\n                image = transform(image).unsqueeze(0)\n                batch_images.append(image)\n            except Exception as e:\n                print(f\"Error loading {path}: {e}\")\n                continue\n        \n        if not batch_images:\n            continue\n            \n        batch_images = torch.cat(batch_images).to(device)\n        \n        with torch.no_grad():\n            features = model(batch_images)[0].squeeze(3).squeeze(2).cpu().numpy()\n            all_features.append(features)\n    \n    if not all_features:\n        raise ValueError(\"No features extracted - check your image paths\")\n    \n    return np.concatenate(all_features)\n\ndef calculate_fid(real_features, generated_features):\n    \"\"\"Calculate FID score between real and generated features\"\"\"\n    mu1, sigma1 = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n    mu2, sigma2 = generated_features.mean(axis=0), np.cov(generated_features, rowvar=False)\n    \n    diff = mu1 - mu2\n    \n    # Calculate sqrt of product of cov matrices\n    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n    \n    # Numerical stability\n    if np.iscomplexobj(covmean):\n        covmean = covmean.real\n    \n    fid = diff.dot(diff) + np.trace(sigma1 + sigma2 - 2 * covmean)\n    return fid\n\n# Calculate both metrics\ndef calculate_metrics(real_image_paths, generated_paths, captions):\n    \"\"\"Calculate both FID and CLIP Score\"\"\"\n    \n    # Calculate FID\n    print(\"Extracting real image features for FID...\")\n    real_features = extract_features_inception(real_image_paths)\n    \n    print(\"Extracting generated image features for FID...\")\n    generated_features = extract_features_inception(generated_paths)\n    \n    print(\"Calculating FID score...\")\n    fid_value = calculate_fid(real_features, generated_features)\n    \n    # Calculate CLIP Score\n    print(\"Calculating CLIP Score...\")\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n    clip_score, all_scores = calculate_clip_score(clip_model, clip_preprocess, generated_paths, captions)\n    \n    return fid_value, clip_score\n\n# Main validation function\ndef main():\n    # Configuration\n    csv_file = \"/kaggle/input/fashion-dataset/val_fixed.csv\"\n    image_folder = \"/kaggle/input/fashion-dataset/val_images\"\n    output_dir = \"generated_val_images\"\n    batch_size = 2  # Conservative for T4 GPU\n    \n    # Setup device\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Using device: {device}\")\n    \n    # Load dataset\n    dataset = ValidationDataset(csv_file, image_folder)\n    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n    \n    # Get all captions and real image paths\n    all_captions = [dataset[i][1] for i in range(len(dataset))]\n    real_image_paths = [os.path.join(image_folder, dataset.data.iloc[i]['image']) \n                       for i in range(len(dataset))]\n    \n    # Verify real images exist\n    for path in real_image_paths[:5]:  # Check first 5\n        if not os.path.exists(path):\n            print(f\"Warning: Real image not found: {path}\")\n    \n    # Load LoRA model\n    print(\"Loading LoRA model...\")\n    pipe = load_model_with_lora()\n    \n    # Generate images\n    print(\"Generating validation images...\")\n    generated_paths = generate_images(pipe, all_captions, output_dir)\n    \n    # Calculate metrics\n    fid_value, clip_score = calculate_metrics(real_image_paths, generated_paths, all_captions)\n    \n    # Save results\n    results = {\n        'FID': fid_value,\n        'CLIP_Score': clip_score,\n        'num_samples': len(dataset)\n    }\n    \n    with open('validation_results.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Metric', 'Value'])\n        for key, value in results.items():\n            writer.writerow([key, value])\n    \n    # Save per-image scores\n    with open('per_image_scores.csv', 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['image_name', 'caption', 'generated_image'])\n        for i in range(len(dataset)):\n            writer.writerow([\n                dataset.data.iloc[i]['image'],\n                dataset.data.iloc[i]['caption'],\n                f\"generated_{i:05d}.png\"\n            ])\n    \n    print(\"\\n\" + \"=\"*50)\n    print(\"VALIDATION RESULTS:\")\n    print(\"=\"*50)\n    print(f\"FID Score: {fid_value:.4f}\")\n    print(f\"CLIP Score: {clip_score:.4f}\")\n    print(f\"Number of samples: {len(dataset)}\")\n    print(\"=\"*50)\n    \n    # Interpretation\n    print(\"\\nINTERPRETATION:\")\n    print(\"- Lower FID is better (0 = perfect)\")\n    print(\"- Higher CLIP Score is better (1.0 = perfect alignment)\")\n    print(f\"- Typical CLIP Scores range from 0.2 to 0.4 for good models\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-19T08:13:51.649487Z","iopub.execute_input":"2025-11-19T08:13:51.650232Z"}},"outputs":[],"execution_count":null}]}