{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/justinpinkney/clip2latent/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:16:48.966949Z",
     "iopub.status.busy": "2025-11-15T16:16:48.966295Z",
     "iopub.status.idle": "2025-11-15T16:16:48.973270Z",
     "shell.execute_reply": "2025-11-15T16:16:48.972287Z",
     "shell.execute_reply.started": "2025-11-15T16:16:48.966878Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%cd clip2latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:16:51.375301Z",
     "iopub.status.busy": "2025-11-15T16:16:51.374814Z",
     "iopub.status.idle": "2025-11-15T16:16:51.380396Z",
     "shell.execute_reply": "2025-11-15T16:16:51.379594Z",
     "shell.execute_reply.started": "2025-11-15T16:16:51.375276Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile custom_requirements.txt\n",
    "\n",
    "wandb==0.12.16\n",
    "ninja==1.10.2.3\n",
    "dalle2-pytorch==0.2.38\n",
    "hydra-core==1.3.2\n",
    "typer==0.4.1\n",
    "joblib==1.1.0\n",
    "webdataset==0.2.5\n",
    "gradio==3.4\n",
    "protobuf==3.20.1\n",
    "-e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:16:54.704997Z",
     "iopub.status.busy": "2025-11-15T16:16:54.704459Z",
     "iopub.status.idle": "2025-11-15T16:18:57.546589Z",
     "shell.execute_reply": "2025-11-15T16:18:57.545762Z",
     "shell.execute_reply.started": "2025-11-15T16:16:54.704970Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -r custom_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T16:19:30.036113Z",
     "iopub.status.busy": "2025-11-15T16:19:30.035567Z",
     "iopub.status.idle": "2025-11-15T16:19:30.044068Z",
     "shell.execute_reply": "2025-11-15T16:19:30.043284Z",
     "shell.execute_reply.started": "2025-11-15T16:19:30.036082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/custom_generate_dataset.py\n",
    "from multiprocessing import Process\n",
    "import multiprocessing as mp\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import typer\n",
    "from joblib import Parallel, delayed\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from clip2latent.models import Clipper, load_sg\n",
    "\n",
    "\n",
    "import multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method('spawn')\n",
    "except:\n",
    "    pass\n",
    "\n",
    "generators = {\n",
    "    \"sg2-ffhq-1024\": partial(load_sg, 'https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan2/versions/1/files/stylegan2-ffhq-1024x1024.pkl'),\n",
    "    \"sg3-lhq-256\": partial(load_sg, 'https://huggingface.co/justinpinkney/stylegan3-t-lhq-256/resolve/main/lhq-256-stylegan3-t-25Mimg.pkl'),\n",
    "    \"my_model\": partial(load_sg, '/kaggle/input/stylegan2-fashion-4000kimg/pytorch/default/1/network-snapshot-004000.pkl')\n",
    "}\n",
    "\n",
    "def mix_styles(w_batch, space):\n",
    "    \"\"\"Defines a style mixing procedure\"\"\"\n",
    "    space_spec = {\n",
    "        \"w3\": (4, 4, 10),\n",
    "    }\n",
    "    latent_mix = space_spec[space]\n",
    "\n",
    "    bs = w_batch.shape[0]\n",
    "    spec = torch.tensor(latent_mix).to(w_batch.device)\n",
    "\n",
    "    index = torch.randint(0,bs, (len(spec),bs)).to(w_batch.device)\n",
    "    return w_batch[index, 0, :].permute(1,0,2).repeat_interleave(spec, dim=1), spec\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_folder_list(\n",
    "    device_index,\n",
    "    out_dir,\n",
    "    generator_name,\n",
    "    feature_extractor_name,\n",
    "    out_image_size,\n",
    "    batch_size,\n",
    "    n_save_workers,\n",
    "    samples_per_folder,\n",
    "    folder_indexes,\n",
    "    space=\"w\",\n",
    "    save_im=True,\n",
    "    ):\n",
    "    \"\"\"Generate a directory of generated images and correspdonding embeddings and latents\"\"\"\n",
    "    latent_dim = 512\n",
    "    device = f\"cuda:{device_index}\"\n",
    "    typer.echo(device_index)\n",
    "\n",
    "    typer.echo(\"Loading generator\")\n",
    "    G = generators[generator_name]().to(device).eval()\n",
    "\n",
    "    typer.echo(\"Loading feature extractor\")\n",
    "    feature_extractor = Clipper(feature_extractor_name).to(device)\n",
    "\n",
    "    typer.echo(\"Generating samples\")\n",
    "    typer.echo(f\"using space {space}\")\n",
    "\n",
    "    with Parallel(n_jobs=n_save_workers, prefer=\"threads\") as parallel:\n",
    "        for i_folder in folder_indexes:\n",
    "            folder_name = out_dir/f\"{i_folder:05d}\"\n",
    "            folder_name.mkdir(exist_ok=True)\n",
    "\n",
    "            z = torch.randn(samples_per_folder, latent_dim, device=device)\n",
    "            w = G.mapping(z, c=None)\n",
    "            ds = torch.utils.data.TensorDataset(w)\n",
    "            loader = torch.utils.data.DataLoader(ds, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "            for batch_idx, batch in enumerate(tqdm(loader, position=device_index)):\n",
    "                if space == \"w\":\n",
    "                    this_w = batch[0].to(device)\n",
    "                    latents = this_w[:,0,:].cpu().numpy()\n",
    "                else:\n",
    "                    this_w, select_idxs = mix_styles(batch[0].to(device), space)\n",
    "                    latents = this_w[:,select_idxs,:].cpu().numpy()\n",
    "\n",
    "                out = G.synthesis(this_w)\n",
    "\n",
    "                out = F.interpolate(out, (out_image_size,out_image_size), mode=\"area\")\n",
    "                image_features = feature_extractor.embed_image(out)\n",
    "                image_features = image_features.cpu().numpy()\n",
    "\n",
    "                if save_im:\n",
    "                    out = out.permute(0,2,3,1).clamp(-1,1)\n",
    "                    out = (255*(out*0.5 + 0.5).cpu().numpy()).astype(np.uint8)\n",
    "                else:\n",
    "                    out = [None]*len(latents)\n",
    "                parallel(\n",
    "                    delayed(process_and_save)(batch_size, folder_name, batch_idx, idx, latent, im, image_feature, save_im)\n",
    "                    for idx, (latent, im, image_feature) in enumerate(zip(latents, out, image_features))\n",
    "                    )\n",
    "\n",
    "    typer.echo(\"finished folder\")\n",
    "\n",
    "\n",
    "def process_and_save(batch_size, folder_name, batch_idx, idx, latent, im, image_feature, save_im):\n",
    "    count = batch_idx*batch_size + idx\n",
    "    basename = folder_name/f\"{folder_name.stem}{count:04}\"\n",
    "    np.save(basename.with_suffix(\".latent.npy\"), latent)\n",
    "    np.save(basename.with_suffix(\".img_feat.npy\"), image_feature)\n",
    "    if save_im:\n",
    "        im = Image.fromarray(im)\n",
    "        im.save(basename.with_suffix(\".gen.jpg\"), quality=95)\n",
    "\n",
    "def make_webdataset(in_dir, out_dir):\n",
    "    import tarfile\n",
    "    in_folders = [x for x in Path(in_dir).glob(\"*\") if x.is_dir]\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir()\n",
    "    for folder in in_folders:\n",
    "        filename = out_dir/f\"{folder.stem}.tar\"\n",
    "        files_to_add = sorted(list(folder.rglob(\"*\")))\n",
    "\n",
    "        with tarfile.open(filename, \"w\") as tar:\n",
    "            for f in files_to_add:\n",
    "                tar.add(f)\n",
    "\n",
    "\n",
    "def main(\n",
    "    out_dir:Path,\n",
    "    n_samples:int=1_000_000,\n",
    "    generator_name:str=\"sg2-ffhq-1024\", # Key into `generators` dict`\n",
    "    feature_extractor_name:str=\"ViT-B/32\",\n",
    "    n_gpus:int=2,\n",
    "    out_image_size:int=256,\n",
    "    batch_size:int=32,\n",
    "    n_save_workers:int=16,\n",
    "    space:str=\"w\",\n",
    "    samples_per_folder:int=10_000,\n",
    "    save_im:bool=False, # Save the generated images?\n",
    "    ):\n",
    "    typer.echo(\"starting\")\n",
    "\n",
    "    out_dir.mkdir(parents=True)\n",
    "\n",
    "    n_folders = math.ceil(n_samples/samples_per_folder)\n",
    "    folder_indexes = range(n_folders)\n",
    "\n",
    "    sub_indexes = np.array_split(folder_indexes, n_gpus)\n",
    "\n",
    "    processes = []\n",
    "    for dev_idx, folder_list in enumerate(sub_indexes):\n",
    "        p = Process(\n",
    "            target=run_folder_list,\n",
    "            args=(\n",
    "                dev_idx,\n",
    "                out_dir,\n",
    "                generator_name,\n",
    "                feature_extractor_name,\n",
    "                out_image_size,\n",
    "                batch_size,\n",
    "                n_save_workers,\n",
    "                samples_per_folder,\n",
    "                folder_list,\n",
    "                space,\n",
    "                save_im,\n",
    "                ),\n",
    "            )\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "\n",
    "    typer.echo(\"finished all\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # mp.set_start_method('spawn')\n",
    "    typer.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:00:09.003033Z",
     "iopub.status.busy": "2025-11-15T17:00:09.002175Z",
     "iopub.status.idle": "2025-11-15T17:19:12.216517Z",
     "shell.execute_reply": "2025-11-15T17:19:12.215896Z",
     "shell.execute_reply.started": "2025-11-15T17:00:09.003002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from scripts.custom_generate_dataset import main  # đổi thành tên file script của bạn, ví dụ: generate_dataset.py\n",
    "\n",
    "# Chạy tạo dataset\n",
    "main(\n",
    "    out_dir=Path(\"/kaggle/working/my_dataset\"),\n",
    "    n_samples=500000,\n",
    "    generator_name=\"my_model\",      # key trong generators dict\n",
    "    feature_extractor_name=\"ViT-B/32\",\n",
    "    n_gpus=1,\n",
    "    out_image_size=256,\n",
    "    batch_size=16,\n",
    "    n_save_workers=8,\n",
    "    space=\"w\",\n",
    "    samples_per_folder=5000,\n",
    "    save_im=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:19:50.328813Z",
     "iopub.status.busy": "2025-11-15T17:19:50.328191Z",
     "iopub.status.idle": "2025-11-15T17:20:23.330631Z",
     "shell.execute_reply": "2025-11-15T17:20:23.330044Z",
     "shell.execute_reply.started": "2025-11-15T17:19:50.328781Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path(\"scripts\").resolve()))\n",
    "\n",
    "from scripts.custom_generate_dataset import make_webdataset\n",
    "make_webdataset(\"/kaggle/working/my_dataset\", \"/kaggle/working/my_dataset_tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:20:46.649079Z",
     "iopub.status.busy": "2025-11-15T17:20:46.648232Z",
     "iopub.status.idle": "2025-11-15T17:20:46.850642Z",
     "shell.execute_reply": "2025-11-15T17:20:46.849885Z",
     "shell.execute_reply.started": "2025-11-15T17:20:46.649048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/working/my_dataset_tar/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:23:25.545456Z",
     "iopub.status.busy": "2025-11-15T17:23:25.544719Z",
     "iopub.status.idle": "2025-11-15T17:23:25.551724Z",
     "shell.execute_reply": "2025-11-15T17:23:25.551094Z",
     "shell.execute_reply.started": "2025-11-15T17:23:25.545427Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/my_config.yaml\n",
    "model:\n",
    "  network:\n",
    "    dim: 512\n",
    "    num_timesteps: 1000\n",
    "    depth: 12\n",
    "    dim_head: 64\n",
    "    heads: 12\n",
    "  diffusion:\n",
    "    image_embed_dim: 512\n",
    "    timesteps: 1000\n",
    "    cond_drop_prob: 0.2\n",
    "    image_embed_scale: 1.0\n",
    "    text_embed_scale: 1.0\n",
    "    beta_schedule: cosine\n",
    "    predict_x_start: true\n",
    "\n",
    "data:\n",
    "  bs: 128\n",
    "  format: webdataset\n",
    "  path: /kaggle/working/my_dataset_tar/{00000..00099}.tar\n",
    "  embed_noise_scale: 1.0\n",
    "  sg_pkl: /kaggle/input/stylegan2-fashion-4000kimg/pytorch/default/1/network-snapshot-004000.pkl\n",
    "  clip_variant: ViT-B/32\n",
    "  n_latents: 1\n",
    "  latent_dim: 512\n",
    "  latent_repeats: \n",
    "  - 14 \n",
    "  val_im_samples: 4\n",
    "  val_text_samples: /kaggle/working/val_text.txt\n",
    "  val_samples_per_text: 1\n",
    "\n",
    "train:\n",
    "  znorm_embed: false\n",
    "  znorm_latent: true\n",
    "  max_it: 100000\n",
    "  val_it: 10000\n",
    "  lr: 1e-4\n",
    "  weight_decay: 0.01\n",
    "  ema_update_every: 10\n",
    "  ema_beta: 0.9999\n",
    "  ema_power: 0.75\n",
    "\n",
    "logging: console\n",
    "wandb_project: clip2latent\n",
    "wandb_entity: null\n",
    "name: null\n",
    "\n",
    "device: cuda\n",
    "resume: ~\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:23:32.018815Z",
     "iopub.status.busy": "2025-11-15T17:23:32.018548Z",
     "iopub.status.idle": "2025-11-15T17:23:32.079622Z",
     "shell.execute_reply": "2025-11-15T17:23:32.079088Z",
     "shell.execute_reply.started": "2025-11-15T17:23:32.018795Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "df = pd.read_csv(\"/kaggle/input/new-text-fashion-blip/data_BLIP_refined.csv\")\n",
    "df[\"caption\"]\n",
    "val_text_path = Path(\"/kaggle/working/val_text.txt\")\n",
    "df[\"caption\"].head(2000).to_csv(val_text_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:23:35.238930Z",
     "iopub.status.busy": "2025-11-15T17:23:35.238207Z",
     "iopub.status.idle": "2025-11-15T17:23:35.246170Z",
     "shell.execute_reply": "2025-11-15T17:23:35.245542Z",
     "shell.execute_reply.started": "2025-11-15T17:23:35.238889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile scripts/custom_train1.py\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import numpy as np\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import wandb\n",
    "from clip2latent.data import load_data\n",
    "from clip2latent.models import load_models\n",
    "from clip2latent.train_utils import (compute_val, make_grid,\n",
    "                                     make_image_val_data, make_text_val_data)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "noop = lambda *args, **kwargs: None\n",
    "logfun = noop\n",
    "\n",
    "class Checkpointer():\n",
    "    \"\"\"A small class to take care of saving checkpoints\"\"\"\n",
    "    def __init__(self, directory, checkpoint_its):\n",
    "        directory = Path(directory)\n",
    "        self.directory = directory\n",
    "        self.checkpoint_its = checkpoint_its\n",
    "        if not directory.exists():\n",
    "            directory.mkdir(parents=True)\n",
    "\n",
    "    def save_checkpoint(self, model, iteration):\n",
    "        if iteration % self.checkpoint_its:\n",
    "            return\n",
    "\n",
    "        k_it = iteration // 1000\n",
    "        filename = self.directory/f\"{k_it:06}.ckpt\"\n",
    "        checkpoint = {\"state_dict\": model.state_dict()}\n",
    "        if hasattr(model, \"cfg\"):\n",
    "            checkpoint[\"cfg\"] = model.cfg\n",
    "\n",
    "        print(f\"Saving checkpoint to {filename}\")\n",
    "        torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "\n",
    "def validation(current_it, device, diffusion_prior, G, clip_model, val_data, samples_per_text):\n",
    "    single_im = {\"clip_features\": val_data[\"val_im\"][\"clip_features\"][0].unsqueeze(0)}\n",
    "    captions = val_data[\"val_caption\"]\n",
    "\n",
    "    for input_data, key, cond_scale, repeats in zip(\n",
    "        [val_data[\"val_im\"], single_im, val_data[\"val_text\"], val_data[\"val_text\"]],\n",
    "        [\"image-similarity\", \"image-vars\", \"text2im\", \"text2im-super2\"],\n",
    "        [1.0, 1.0, 1.0, 2.0],\n",
    "        [1, 8, samples_per_text, samples_per_text],\n",
    "    ):\n",
    "        tiled_data = input_data[\"clip_features\"].repeat_interleave(repeats, dim=0)\n",
    "        cos_sim, ims = compute_val(diffusion_prior, tiled_data, G, clip_model, device, cond_scale=cond_scale)\n",
    "        logfun({f'val/{key}':cos_sim.mean()}, step=current_it)\n",
    "\n",
    "\n",
    "        if key.startswith(\"text\"):\n",
    "            num_chunks = int(np.ceil(ims.shape[0]//repeats))\n",
    "            for idx, (sim, im_chunk) in enumerate(zip(\n",
    "                cos_sim.chunk(num_chunks),\n",
    "                ims.chunk(num_chunks)\n",
    "                )):\n",
    "\n",
    "                caption = captions[idx]\n",
    "                im = wandb.Image(make_grid(im_chunk), caption=f'{sim.mean():.2f} - {caption}')\n",
    "                logfun({f'val/image/{key}/{idx}': im}, step=current_it)\n",
    "        else:\n",
    "            for idx, im in enumerate(ims.chunk(int(np.ceil(ims.shape[0]/16)))):\n",
    "                logfun({f'val/image/{key}/{idx}': wandb.Image(make_grid(im))}, step=current_it)\n",
    "\n",
    "    logger.info(\"Validation done.\")\n",
    "\n",
    "def train_step(diffusion_prior, device, batch):\n",
    "    diffusion_prior.train()\n",
    "    batch_z, batch_w = batch\n",
    "    batch_z = batch_z.to(device)\n",
    "    batch_w = batch_w.to(device)\n",
    "\n",
    "    loss = diffusion_prior(batch_z, batch_w)\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train(trainer, loader, device, val_it, validate, save_checkpoint, max_it, print_it=50):\n",
    "\n",
    "    current_it = 0\n",
    "    current_epoch = 0\n",
    "\n",
    "    while current_it < max_it:\n",
    "\n",
    "        logfun({'epoch': current_epoch}, step=current_it)\n",
    "        pbar = tqdm(loader)\n",
    "        for batch in pbar:\n",
    "            if current_it % val_it == 0:\n",
    "                with torch.no_grad():\n",
    "                    validate(current_it, device, trainer)\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "                #validate(current_it)\n",
    "\n",
    "\n",
    "            trainer.train()\n",
    "            batch_clip, batch_latent = batch\n",
    "\n",
    "            input_args = {\n",
    "                \"image_embed\": batch_latent.to(device),\n",
    "                \"text_embed\": batch_clip.to(device)\n",
    "            }\n",
    "            loss = trainer(**input_args)\n",
    "\n",
    "            if (current_it % print_it == 0):\n",
    "                logfun({'loss': loss}, step=current_it)\n",
    "\n",
    "            trainer.update()\n",
    "            current_it += 1\n",
    "            pbar.set_postfix({\"loss\": loss, \"epoch\": current_epoch, \"it\": current_it})\n",
    "\n",
    "            save_checkpoint(trainer, current_it)\n",
    "\n",
    "        current_epoch += 1\n",
    "\n",
    "\n",
    "@hydra.main(config_path=\"../config\", config_name=\"config\")\n",
    "def main(cfg):\n",
    "\n",
    "    if cfg.logging == \"wandb\":\n",
    "        wandb.init(\n",
    "            project=cfg.wandb_project,\n",
    "            config=OmegaConf.to_container(cfg),\n",
    "            entity=cfg.wandb_entity,\n",
    "            name=cfg.name,\n",
    "        )\n",
    "        global logfun\n",
    "        logfun = wandb.log\n",
    "    elif cfg.logging is None:\n",
    "        logger.info(\"Not logging\")\n",
    "    elif cfg.logging == \"console\" or cfg.logging is None:\n",
    "        # log ra console\n",
    "        logfun = lambda d, step=None: print(f\"[Step {step}] {d}\" if step is not None else d)\n",
    "        print(\"Console logging enabled.\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"Logging type {cfg.logging} not implemented\")\n",
    "\n",
    "    device = cfg.device\n",
    "    stats, loader = load_data(cfg.data)\n",
    "    G, clip_model, trainer = load_models(cfg, device, stats)\n",
    "\n",
    "    text_embed, text_samples = make_text_val_data(G, clip_model, hydra.utils.to_absolute_path(cfg.data.val_text_samples))\n",
    "    val_data = {\n",
    "        \"val_im\": make_image_val_data(G, clip_model, cfg.data.val_im_samples, device),\n",
    "        \"val_text\": text_embed,\n",
    "        \"val_caption\": text_samples,\n",
    "    }\n",
    "\n",
    "    if 'resume' in cfg and cfg.resume is not None:\n",
    "        # Does not load previous iteration count\n",
    "        logger.info(f\"Resuming from {cfg.resume}\")\n",
    "        trainer.load_state_dict(torch.load(cfg.resume, map_location=\"cpu\")[\"state_dict\"])\n",
    "\n",
    "    checkpoint_dir = f\"checkpoints/{datetime.now():%Y%m%d-%H%M%S}\"\n",
    "    checkpointer = Checkpointer(checkpoint_dir, cfg.train.val_it)\n",
    "    validate1 = partial(validation,\n",
    "        G=G,\n",
    "        clip_model=clip_model,\n",
    "        val_data=val_data,\n",
    "        samples_per_text=cfg.data.val_samples_per_text,\n",
    "        )\n",
    "\n",
    "\n",
    "    train(trainer, loader, device,\n",
    "        val_it=cfg.train.val_it,\n",
    "        max_it=cfg.train.max_it,\n",
    "        validate=validate1,\n",
    "        save_checkpoint=checkpointer.save_checkpoint,\n",
    "        )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-15T17:28:49.993356Z",
     "iopub.status.busy": "2025-11-15T17:28:49.993091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.custom_train1 import main as train_main  # import function main train\n",
    "\n",
    "cfg_path = \"/kaggle/working/my_config.yaml\"\n",
    "cfg = OmegaConf.load(cfg_path)\n",
    "print(cfg)\n",
    "# Chạy train\n",
    "train_main(cfg)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8735877,
     "sourceId": 13730587,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 498672,
     "modelInstanceId": 483121,
     "sourceId": 640576,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
